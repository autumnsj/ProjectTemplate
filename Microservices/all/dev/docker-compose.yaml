version: "3.8"
services:
  nacos:
    image: nacos/nacos-server:${NACOS_VERSION}
    container_name: nacos
    env_file:
      - ./env/custom-application-config.env
    volumes:
      - ./volumes/nacos/:/home/nacos/logs
      - ./init.d/application.properties:/home/nacos/conf/application.properties
    networks:
      dev_net:
        ipv4_address: ${NACOS_IP}
    ports:
      - "8848:8848"
      - "9848:9848"
    depends_on:
      mysql:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "echo 'ruok' | curl -s telnet://localhost:8848 || exit 1"]
      retries: 10
    restart: on-failure
  mysql:
    container_name: mysql
    build:
      context: images/mysql
      dockerfile: Dockerfile
    env_file:
      - ./env/mysql.env
    volumes:
      - ./volumes/mysql:/var/lib/mysql
    ports:
      - "3306:3306"
    healthcheck:
      test: [ "CMD", "mysqladmin" ,"ping", "-h", "localhost" ]
      interval: 5s
      timeout: 10s
      retries: 10
    networks:
      dev_net:
        ipv4_address: ${MYSQL_IP}
  seata:
    container_name: seata
    # build: 
    #   context: ../../seata/1.7.03
    #   dockerfile: Dockerfile
    image: seataio/seata-server
    ports:
      - "7091:7091"
      - "8091:8091"
    environment:
      # 以SEATA_IP作为host注册seata server
      - SEATA_IP=${LOCAL_IP}
      - SEATA_PORT=8091
    volumes:
      # - "/usr/share/zoneinfo/Asia/Shanghai:/etc/localtime"        #设置系统时区
      # - "/usr/share/zoneinfo/Asia/Shanghai:/etc/timezone"  #设置时区
      # 假设我们通过docker cp命令把资源文件拷贝到相对路径`./seata-server/resources`中
      # 如有问题，请阅读上面的[注意事项]以及[使用自定义配置文件]
      - "./config/seata/application.yml:/seata-server/conf/application.yml"
    depends_on:
      nacos:
        condition: service_healthy
    networks:
      dev_net:
  cache:
    container_name: cache
    image: redis
    restart: always
    ports:
      - '6379:6379'
    #command: redis-server --save 20 1 --loglevel warning --requirepass eYVX7EwVmmxKPCDmwMtyKVge8oLd2t81
    volumes: 
      - "./volumes/cache:/data"
    networks:
      dev_net:
  rocketmq-namesrv:
    image: apache/rocketmq
    container_name: rocketmq-namesrv
    environment:
      - JAVA_OPT_EXT=-server -Xms128m -Xmx128m
    ports:
      - 9876:9876
    volumes:
      - ./volumes/mq/namesrv/logs:/home/rocketmq/logs
    command: sh mqnamesrv
    networks:
      dev_net:
  #Service for broker
  rocketmq-broker:
    image: apache/rocketmq
    container_name: rocketmq-broker
    links:
      - rocketmq-namesrv
    ports:
      - 10909:10909
      - 10911:10911
      - 10912:10912
    environment:
      - NAMESRV_ADDR=rocketmq-namesrv:9876
      - JAVA_OPT_EXT=-server -Xms128m -Xmx128m
    volumes:
      - ./volumes/mq/broker/logs:/home/rocketmq/logs
      - ./volumes/mq/broker/store:/home/rocketmq/store
      - ./config/rocketmq/broker-a.conf:/opt/rocketmq-ROCKETMQ_VERSION/conf/broker.conf
    command: sh mqbroker -c /opt/rocketmq-ROCKETMQ_VERSION/conf/broker.conf
    networks:
      dev_net:
  rocketmq-console:
    image: styletang/rocketmq-console-ng
    container_name: rocketmq-console
    links:
      - rocketmq-namesrv
    ports:
      - 8081:8080
    environment:
        JAVA_OPTS: "-Drocketmq.namesrv.addr=rocketmq-namesrv:9876 -Dcom.rocketmq.sendMessageWithVIPChannel=false"
    networks:
      dev_net:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch-oss:${ES_VERSION}
    container_name: elasticsearch
    mem_limit: 1g
    ports:
      - "9200:9200"
    healthcheck:
      test: [ "CMD-SHELL", "curl --silent --fail localhost:9200/_cluster/health || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    environment:
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    networks:
      dev_net:

  skywalking-oap:
    image: ${OAP_IMAGE}
    container_name: skywalking-oap
    depends_on:
      elasticsearch:
        condition: service_healthy
    links:
      - elasticsearch
    ports:
      - "11800:11800"
      - "12800:12800"
    healthcheck:
      test: [ "CMD-SHELL", "/skywalking/bin/swctl ch" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    environment:
      SW_STORAGE: elasticsearch
      SW_STORAGE_ES_CLUSTER_NODES: elasticsearch:9200
      SW_HEALTH_CHECKER: default
      SW_TELEMETRY: prometheus
      JAVA_OPTS: "-Xms2048m -Xmx2048m"
    networks:
      dev_net:
  skywalking-ui:
    image: ${UI_IMAGE}
    container_name: skywalking-ui
    depends_on:
      skywalking-oap:
        condition: service_healthy
    links:
      - skywalking-oap
    ports:
      - "8082:8080"
    environment:
      SW_OAP_ADDRESS: http://skywalking-oap:12800
      SW_ZIPKIN_ADDRESS: http://skywalking-oap:9412
    networks:
      dev_net:
  sentinel:
    container_name: sentinel
    build:
      context: ../../sentinel
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    environment:
      JAVA_OPTIONS: -Dnacos.serverAddr=nacos:8848
    networks:
      dev_net:
    depends_on:
      nacos:
        condition: service_healthy
  xxl-job:
    image: xuxueli/xxl-job-admin:2.0.2
    restart: always
    container_name: xxl-job
    ports:
      - 28888:8080
    links:
      - mysql
    volumes:
      - "./volumes/xxl-job:/data/applogs"
    environment:
      - spring.datasource.url=jdbc:mysql://mysql:3306/xxl-job?Unicode=true&characterEncoding=UTF-8&autoReconnect=true
      - spring.datasource.password=root
    depends_on: 
      mysql:
        condition: service_healthy
    networks:
      dev_net:
networks:
  dev_net:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.0.0.0/24
